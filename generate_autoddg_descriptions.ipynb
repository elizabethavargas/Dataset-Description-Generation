{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMgQN2exDt2yAShwb1GN19m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elizabethavargas/Dataset-Description-Generation/blob/main/generate_autoddg_descriptions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AutoDDG Descriptions\n",
        "Here were generate descriptions for a random sample of 500 NYC Open Data datasets using AutoDDG which will be used as a baseline for our model."
      ],
      "metadata": {
        "id": "-zhgvmpDBiYA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/VIDA-NYU/AutoDDG@main\n",
        "\n",
        "import pandas as pd\n",
        "from openai import OpenAI\n",
        "\n",
        "from autoddg import AutoDDG, GPTEvaluator\n",
        "from autoddg.utils import get_sample"
      ],
      "metadata": {
        "id": "xbUDtDG0hyOR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56b0b5cc-5f9d-497f-a7e1-fa387f0cce69"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/VIDA-NYU/AutoDDG@main\n",
            "  Cloning https://github.com/VIDA-NYU/AutoDDG (to revision main) to /tmp/pip-req-build-go3xv_jt\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/VIDA-NYU/AutoDDG /tmp/pip-req-build-go3xv_jt\n",
            "  Resolved https://github.com/VIDA-NYU/AutoDDG to commit 5f26a43b216d2ee4079b8e4969397a12d414db95\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: datamart_profiler==0.11 in /usr/local/lib/python3.12/dist-packages (from autoddg==0.1.0.dev0) (0.11)\n",
            "Requirement already satisfied: fastembed==0.5.1 in /usr/local/lib/python3.12/dist-packages (from autoddg==0.1.0.dev0) (0.5.1)\n",
            "Requirement already satisfied: nltk==3.9.1 in /usr/local/lib/python3.12/dist-packages (from autoddg==0.1.0.dev0) (3.9.1)\n",
            "Requirement already satisfied: numpy>=2.2.2 in /usr/local/lib/python3.12/dist-packages (from autoddg==0.1.0.dev0) (2.3.5)\n",
            "Requirement already satisfied: openai==1.61.0 in /usr/local/lib/python3.12/dist-packages (from autoddg==0.1.0.dev0) (1.61.0)\n",
            "Requirement already satisfied: pandas>=2.2.3 in /usr/local/lib/python3.12/dist-packages (from autoddg==0.1.0.dev0) (2.3.3)\n",
            "Requirement already satisfied: rank-bm25==0.2.2 in /usr/local/lib/python3.12/dist-packages (from autoddg==0.1.0.dev0) (0.2.2)\n",
            "Requirement already satisfied: scikit-learn==1.6.1 in /usr/local/lib/python3.12/dist-packages (from autoddg==0.1.0.dev0) (1.6.1)\n",
            "Requirement already satisfied: beartype>=0.18.5 in /usr/local/lib/python3.12/dist-packages (from autoddg==0.1.0.dev0) (0.22.6)\n",
            "Requirement already satisfied: pyyaml==6.0.2 in /usr/local/lib/python3.12/dist-packages (from autoddg==0.1.0.dev0) (6.0.2)\n",
            "Requirement already satisfied: opentelemetry-api in /usr/local/lib/python3.12/dist-packages (from datamart_profiler==0.11->autoddg==0.1.0.dev0) (1.37.0)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.12/dist-packages (from datamart_profiler==0.11->autoddg==0.1.0.dev0) (0.23.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.12/dist-packages (from datamart_profiler==0.11->autoddg==0.1.0.dev0) (2.9.0.post0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from datamart_profiler==0.11->autoddg==0.1.0.dev0) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from datamart_profiler==0.11->autoddg==0.1.0.dev0) (2.32.4)\n",
            "Requirement already satisfied: datamart-geo<0.4,>=0.2.3 in /usr/local/lib/python3.12/dist-packages (from datamart_profiler==0.11->autoddg==0.1.0.dev0) (0.3.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.20 in /usr/local/lib/python3.12/dist-packages (from fastembed==0.5.1->autoddg==0.1.0.dev0) (0.36.0)\n",
            "Requirement already satisfied: loguru<0.8.0,>=0.7.2 in /usr/local/lib/python3.12/dist-packages (from fastembed==0.5.1->autoddg==0.1.0.dev0) (0.7.3)\n",
            "Requirement already satisfied: mmh3<5.0.0,>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from fastembed==0.5.1->autoddg==0.1.0.dev0) (4.1.0)\n",
            "Requirement already satisfied: onnxruntime!=1.20.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from fastembed==0.5.1->autoddg==0.1.0.dev0) (1.23.2)\n",
            "Requirement already satisfied: pillow<11.0.0,>=10.3.0 in /usr/local/lib/python3.12/dist-packages (from fastembed==0.5.1->autoddg==0.1.0.dev0) (10.4.0)\n",
            "Requirement already satisfied: py-rust-stemmers<0.2.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from fastembed==0.5.1->autoddg==0.1.0.dev0) (0.1.5)\n",
            "Requirement already satisfied: tokenizers<1.0,>=0.15 in /usr/local/lib/python3.12/dist-packages (from fastembed==0.5.1->autoddg==0.1.0.dev0) (0.22.1)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.66 in /usr/local/lib/python3.12/dist-packages (from fastembed==0.5.1->autoddg==0.1.0.dev0) (4.67.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk==3.9.1->autoddg==0.1.0.dev0) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk==3.9.1->autoddg==0.1.0.dev0) (1.5.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai==1.61.0->autoddg==0.1.0.dev0) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai==1.61.0->autoddg==0.1.0.dev0) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai==1.61.0->autoddg==0.1.0.dev0) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai==1.61.0->autoddg==0.1.0.dev0) (0.12.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai==1.61.0->autoddg==0.1.0.dev0) (2.12.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai==1.61.0->autoddg==0.1.0.dev0) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai==1.61.0->autoddg==0.1.0.dev0) (4.15.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.6.1->autoddg==0.1.0.dev0) (1.16.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.6.1->autoddg==0.1.0.dev0) (3.6.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.3->autoddg==0.1.0.dev0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.3->autoddg==0.1.0.dev0) (2025.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai==1.61.0->autoddg==0.1.0.dev0) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai==1.61.0->autoddg==0.1.0.dev0) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai==1.61.0->autoddg==0.1.0.dev0) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.61.0->autoddg==0.1.0.dev0) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.20->fastembed==0.5.1->autoddg==0.1.0.dev0) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.20->fastembed==0.5.1->autoddg==0.1.0.dev0) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.20->fastembed==0.5.1->autoddg==0.1.0.dev0) (25.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.20->fastembed==0.5.1->autoddg==0.1.0.dev0) (1.2.0)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.12/dist-packages (from onnxruntime!=1.20.0,>=1.17.0->fastembed==0.5.1->autoddg==0.1.0.dev0) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime!=1.20.0,>=1.17.0->fastembed==0.5.1->autoddg==0.1.0.dev0) (25.9.23)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime!=1.20.0,>=1.17.0->fastembed==0.5.1->autoddg==0.1.0.dev0) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime!=1.20.0,>=1.17.0->fastembed==0.5.1->autoddg==0.1.0.dev0) (1.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai==1.61.0->autoddg==0.1.0.dev0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai==1.61.0->autoddg==0.1.0.dev0) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai==1.61.0->autoddg==0.1.0.dev0) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil->datamart_profiler==0.11->autoddg==0.1.0.dev0) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->datamart_profiler==0.11->autoddg==0.1.0.dev0) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->datamart_profiler==0.11->autoddg==0.1.0.dev0) (2.5.0)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api->datamart_profiler==0.11->autoddg==0.1.0.dev0) (8.7.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api->datamart_profiler==0.11->autoddg==0.1.0.dev0) (3.23.0)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.12/dist-packages (from coloredlogs->onnxruntime!=1.20.0,>=1.17.0->fastembed==0.5.1->autoddg==0.1.0.dev0) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime!=1.20.0,>=1.17.0->fastembed==0.5.1->autoddg==0.1.0.dev0) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run AutoDDG on a random sample of datasets\n"
      ],
      "metadata": {
        "id": "5HYN3ONk5Vhm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import pickle\n",
        "\n",
        "# take random sample of dataserts\n",
        "datasets = pd.read_pickle('datasets.pkl')\n",
        "sampled_datasets = random.sample(datasets, 500)\n",
        "\n",
        "with open('sampled_datasets.pkl', 'wb') as f:\n",
        "    pickle.dump(sampled_datasets, f)"
      ],
      "metadata": {
        "id": "V-PRVL0r4BhB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create model object\n",
        "key = KEY\n",
        "client = OpenAI(\n",
        "            api_key=key, base_url=\"https://api.deepinfra.com/v1/openai\"\n",
        "        )\n",
        "auto_ddg = AutoDDG(client=client, model_name=\"meta-llama/Meta-Llama-3-8B-Instruct\", description_temperature=0.0)"
      ],
      "metadata": {
        "id": "HLpm-DJZ_J5O"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import concurrent.futures\n",
        "import pandas as pd\n",
        "import time\n",
        "from openai import OpenAI\n",
        "from autoddg import AutoDDG\n",
        "from autoddg.utils import get_sample\n",
        "\n",
        "\n",
        "def process_single_dataset(dataset_entry, api_key, max_retries=3, retry_delay=2):\n",
        "    \"\"\"Process one dataset safely in an isolated process.\"\"\"\n",
        "\n",
        "    # retry loop to handle rate-limit errors or corrupted responses\n",
        "    for attempt in range(1, max_retries + 1):\n",
        "        try:\n",
        "            # Create fresh, isolated client\n",
        "            local_client = OpenAI(\n",
        "                api_key=api_key,\n",
        "                base_url=\"https://api.deepinfra.com/v1/openai\"\n",
        "            )\n",
        "\n",
        "            local_auto_ddg = AutoDDG(\n",
        "                client=local_client,\n",
        "                model_name=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
        "                description_temperature=0.0\n",
        "            )\n",
        "\n",
        "            # extract dataset info\n",
        "            title = dataset_entry[\"dataset_name\"]\n",
        "            original_description = dataset_entry[\"description\"]\n",
        "            data_dict = dataset_entry[\"data_example\"]\n",
        "            dataset_id = dataset_entry[\"dataset_id\"]\n",
        "\n",
        "            # dataframe\n",
        "            data_df = pd.DataFrame([data_dict])\n",
        "\n",
        "            # sample rows\n",
        "            sample_df, dataset_sample = get_sample(data_df, sample_size=1)\n",
        "\n",
        "            # profiles\n",
        "            basic_profile, structural_profile = local_auto_ddg.profile_dataframe(data_df)\n",
        "            semantic_profile_details = local_auto_ddg.analyze_semantics(sample_df)\n",
        "\n",
        "            semantic_profile = \"\\n\".join(\n",
        "                section for section in [structural_profile, semantic_profile_details]\n",
        "                if section\n",
        "            )\n",
        "\n",
        "            # Topic\n",
        "            data_topic = local_auto_ddg.generate_topic(\n",
        "                title=title,\n",
        "                original_description=original_description,\n",
        "                dataset_sample=dataset_sample,\n",
        "            )\n",
        "\n",
        "            # Description\n",
        "            prompt, description = local_auto_ddg.describe_dataset(\n",
        "                dataset_sample=dataset_sample,\n",
        "                dataset_profile=basic_profile,\n",
        "                use_profile=True,\n",
        "                semantic_profile=semantic_profile,\n",
        "                use_semantic_profile=True,\n",
        "                data_topic=data_topic,\n",
        "                use_topic=True,\n",
        "            )\n",
        "\n",
        "            # SUCCESS\n",
        "            return {\n",
        "                \"id\": dataset_id,\n",
        "                \"prompt\": prompt,\n",
        "                \"description\": description,\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(\n",
        "                f\"[{dataset_entry['dataset_id']}] Attempt {attempt}/{max_retries} failed: {e}\"\n",
        "            )\n",
        "            if attempt == max_retries:\n",
        "                # give up\n",
        "                return {\n",
        "                    \"id\": dataset_entry[\"dataset_id\"],\n",
        "                    \"error\": str(e),\n",
        "                    \"prompt\": None,\n",
        "                    \"description\": None\n",
        "                }\n",
        "\n",
        "            time.sleep(retry_delay)  # backoff\n",
        "\n",
        "\n",
        "processed_results = []\n",
        "\n",
        "with concurrent.futures.ProcessPoolExecutor(max_workers=10) as executor:\n",
        "    future_to_dataset = {\n",
        "        executor.submit(process_single_dataset, dataset, key): dataset\n",
        "        for dataset in sampled_datasets\n",
        "    }\n",
        "\n",
        "    for future in concurrent.futures.as_completed(future_to_dataset):\n",
        "        dataset = future_to_dataset[future]\n",
        "        result = future.result()\n",
        "        processed_results.append(result)\n",
        "\n",
        "# Summary output\n",
        "print(f\"Processed {len(processed_results)} datasets.\\n\")\n",
        "\n",
        "# Show first successful result\n",
        "first_ok = next((r for r in processed_results if r.get(\"prompt\")), None)\n",
        "if first_ok:\n",
        "    print(\"First processed dataset with valid results:\")\n",
        "    print(first_ok)\n",
        "else:\n",
        "    print(\"No successful dataset found.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBLOCYKTJyxw",
        "outputId": "ab448bcf-9cc2-48c3-b7a1-aeef7f704600"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[r6ub-zhff] Attempt 1/3 failed: 'dict' object has no attribute 'lower'\n",
            "[7umd-hdjb] Attempt 1/3 failed: 'bool' object has no attribute 'get'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datamart_profiler.core:Unmatched latitude columns: ['residents_hispanic_or_latino']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[rdjw-z878] Attempt 1/3 failed: 'bool' object has no attribute 'get'\n",
            "[r6ub-zhff] Attempt 2/3 failed: 'str' object has no attribute 'get'\n",
            "[m64b-i6yz] Attempt 1/3 failed: 'bool' object has no attribute 'get'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datamart_profiler.core:Unmatched latitude columns: ['residents_hispanic_or_latino']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[vf4p-p8ui] Attempt 1/3 failed: 'bool' object has no attribute 'get'\n",
            "[ya22-5bh7] Attempt 1/3 failed: 'NoneType' object has no attribute 'get'\n",
            "[ucdy-byxd] Attempt 1/3 failed: 'dict' object has no attribute 'lower'\n",
            "[wtqm-fd2z] Attempt 1/3 failed: 'str' object has no attribute 'get'\n",
            "[rhe8-mgbb] Attempt 1/3 failed: 'dict' object has no attribute 'lower'\n",
            "[pjs3-c3z5] Attempt 1/3 failed: 'bool' object has no attribute 'get'\n",
            "[r6ub-zhff] Attempt 3/3 failed: Error code: 400 - {'error': {'message': 'Input too long'}}\n",
            "[usc3-8zwd] Attempt 1/3 failed: 'str' object has no attribute 'get'\n",
            "[yeba-ynb5] Attempt 1/3 failed: Error code: 400 - {'error': {'message': 'Input too long'}}\n",
            "[5kqf-fg3n] Attempt 1/3 failed: 'dict' object has no attribute 'lower'\n",
            "[usc3-8zwd] Attempt 2/3 failed: 'dict' object has no attribute 'lower'\n",
            "[tyv9-j3ti] Attempt 1/3 failed: 'dict' object has no attribute 'lower'\n",
            "[yeba-ynb5] Attempt 2/3 failed: Error code: 400 - {'error': {'message': 'Input too long'}}\n",
            "[m8p6-tp4b] Attempt 1/3 failed: 'bool' object has no attribute 'get'\n",
            "[kvfi-kxcq] Attempt 1/3 failed: 'str' object has no attribute 'get'\n",
            "[9ksk-35jf] Attempt 1/3 failed: 'dict' object has no attribute 'lower'\n",
            "[6bgk-3dad] Attempt 1/3 failed: 'bool' object has no attribute 'get'\n",
            "[usc3-8zwd] Attempt 3/3 failed: Error code: 400 - {'error': {'message': 'Input too long'}}\n",
            "[yeba-ynb5] Attempt 3/3 failed: Error code: 400 - {'error': {'message': 'Input too long'}}\n",
            "[utpj-74fz] Attempt 1/3 failed: 'str' object has no attribute 'get'\n",
            "[utpj-74fz] Attempt 2/3 failed: Error code: 400 - {'error': {'message': 'Input too long'}}\n",
            "[utpj-74fz] Attempt 3/3 failed: 'dict' object has no attribute 'lower'\n",
            "Processed 500 datasets.\n",
            "\n",
            "First processed dataset with valid results:\n",
            "{'id': 'thrx-b6bc', 'prompt': 'Answer the question using the following information.\\n\\nFirst, consider the dataset sample:\\n\\nneighborhood,type_of_home,number_of_sales,lowest_sale_price,average_sale_price,median_sale_price,highest_sale_price\\nBATH BEACH               ,01 ONE FAMILY HOMES                        ,17,400000,580235,530000,1135000\\n\\n\\nAdditionally, the dataset profile is as follows:\\n\\nThe key data profile information for this dataset includes:\\n**neighborhood**: Data is of type text. There are 1 unique values. \\n**type_of_home**: Data is of type text. \\n**number_of_sales**: Data is of type integer. There are 1 unique values. Coverage spans from 0 to 17.0. \\n**lowest_sale_price**: Data is of type integer. There are 1 unique values. Coverage spans from 0 to 400000.0. \\n**average_sale_price**: Data is of type integer. There are 1 unique values. Coverage spans from 0 to 580235.0. \\n**median_sale_price**: Data is of type integer. There are 1 unique values. Coverage spans from 0 to 530000.0. \\n**highest_sale_price**: Data is of type integer. There are 1 unique values. Coverage spans from 0 to 1135000.0. \\n\\nBased on this profile, please add sentence(s) to enrich the dataset description.\\n\\nFurthermore, the semantic profile of the dataset columns is as follows:\\nThe key semantic information for this dataset includes:\\n**neighborhood**: Represents location. Contains spatial data (resolution: Location). Domain-specific type: general. Function/Usage context: filtering/grouping. \\n**type_of_home**: Represents thing. \\n**number_of_sales**: Represents measurement. Function/Usage context: aggregation key. \\n**lowest_sale_price**: Represents scalar value. Domain-specific type: e-commerce. Function/Usage context: aggregation key. \\n**average_sale_price**: Domain-specific type: e-commerce. Function/Usage context: aggregation key. \\n**median_sale_price**: Represents financial value. Domain-specific type: e-commerce. Function/Usage context: aggregation key. \\n**highest_sale_price**: Represents financial data. Domain-specific type: e-commerce. Function/Usage context: aggregation key. \\n\\nBased on this information, please add sentence(s) discussing the semantic profile in the description.\\n\\nMoreover, the dataset topic is: Brooklyn Home Sales. Based on this topic, please add sentence(s) describing what this dataset can be used for.\\n\\nQuestion: Based on the information above and the requirements, provide a dataset description in sentences. Use only natural, readable sentences without special formatting.\\n\\nAnswer:\\n\\nTarget length: approximately 100 words.', 'description': 'Here is a rewritten dataset description that incorporates the provided information:\\n\\nThis dataset, Brooklyn Home Sales, contains information about home sales in the BATH BEACH neighborhood. It provides a snapshot of the real estate market, with data on the type of homes sold, the number of sales, and the prices of those sales. The dataset includes six columns: neighborhood, type of home, number of sales, lowest sale price, average sale price, and highest sale price. The semantic profile of the dataset reveals that the neighborhood column represents a location, while the other columns represent various types of data, including measurements, scalar values, and financial data. This dataset can be used to analyze and understand the Brooklyn home sales market, providing valuable insights for real estate professionals, researchers, and anyone interested in the local housing market.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save results\n",
        "with open('autoddg_results.pkl', 'wb') as f:\n",
        "    pickle.dump(processed_results, f)"
      ],
      "metadata": {
        "id": "LDKx4-_tKBq5"
      },
      "execution_count": 27,
      "outputs": []
    }
  ]
}