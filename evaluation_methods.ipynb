{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOYJ2AzlaMvRdCxuksx+ejx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elizabethavargas/Dataset-Description-Generation/blob/main/evaluation_methods.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Evaluator Class"
      ],
      "metadata": {
        "id": "zlMDiT5_amD7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_system_message = \"You are a helpful and precise assistant for checking the quality of the dataset description.\"\n",
        "evaluation_prompt = \"\"\"You will be given one tabular dataset description. Your task is to rate the description on 3 metrics.\n",
        "    Please make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed.\n",
        "\n",
        "    Evaluation Criteria:\n",
        "    1. Completeness (1-10) - Evaluates how thoroughly the dataset description covers essential aspects such as the scope of data, query workloads, summary statistics, and possible tasks or applications.\n",
        "    A high score indicates that the description provides a comprehensive overview, including details on dataset size, structure, fields, and potential use cases.\n",
        "    2. Conciseness (1-10) - Measures the efficiency of the dataset description in conveying necessary information without redundancy.\n",
        "    A high score indicates that the description is succinct, avoiding unnecessary details while employing semantic types (e.g., categories, entities) to streamline communication.\n",
        "    3. Readability (1-10) -  Evaluates the logical flow and readability of the dataset description.\n",
        "    A high score suggests that the description progresses logically from one section to the next, creating a coherent and integrated narrative that facilitates understanding of the dataset.\n",
        "\n",
        "    Evaluation Steps:\n",
        "    Read the dataset description carefully and identify the main topic and key points. Assign a score for each criterion on a scale of 1 to 10, where 1 is the lowest and 10 is the highest based on the Evaluation Criteria.\n",
        "\n",
        "    Example 1:\n",
        "    Description: The dataset provides information on alcohol-impaired driving deaths and occupant deaths across various states in the United States. It includes data for 51 states, detailing the number of alcohol-impaired driving deaths and occupant deaths, with values ranging from 0 to 3723 and 0 to 10406, respectively. Each entry also contains the state abbreviation and its geographical coordinates. The dataset is structured with categorical and numerical data types, focusing on traffic safety and casualty statistics. Key attributes include state names, death counts, and location coordinates, making it a valuable resource for analyzing traffic safety trends and issues related to impaired driving.\n",
        "    Evaluation Form (scores ONLY): Completeness: 7, Conciseness: 9, Readability: 9\n",
        "\n",
        "    Example 2:\n",
        "    Description: The dataset provides a comprehensive overview of traffic safety statistics across various states in the United States, specifically focusing on alcohol-impaired driving deaths and occupant deaths. It includes data from 51 unique states, represented by their two-letter postal abbreviations, such as MA (Massachusetts), SD (South Dakota), AK (Alaska), MS (Mississippi), and ME (Maine). Each entry in the dataset captures critical information regarding the number of alcohol-impaired driving deaths and the total occupant deaths resulting from traffic incidents.\n",
        "    The column \"Alcohol-Impaired Driving Deaths\" is represented as an integer, indicating the number of fatalities attributed to alcohol impairment while driving. The dataset reveals a range of values, with the highest recorded number being 2367 deaths in Mississippi, highlighting the severity of the issue in certain regions. In contrast, states like Alaska report significantly lower figures, with only 205 alcohol-impaired driving deaths.\n",
        "    The \"Occupant Deaths\" column also consists of integer values, representing the total number of deaths among vehicle occupants, regardless of the cause. This data spans from 0 to 10406, with Mississippi again showing the highest number of occupant deaths at 6100, which raises concerns about overall traffic safety in the state.\n",
        "    Additionally, the dataset includes a \"Location\" column that provides geographical coordinates for each state, enhancing the spatial understanding of the data. The coordinates are formatted as latitude and longitude pairs, allowing for potential mapping and geographical analysis of traffic safety trends.\n",
        "    Overall, this dataset serves as a valuable resource for researchers, policymakers, and public safety advocates aiming to understand and address the impact of alcohol on driving safety across different states. It highlights the need for targeted interventions and policies to reduce alcohol-impaired driving incidents and improve occupant safety on the roads.\n",
        "    Evaluation Form (scores ONLY): Completeness: 8, Conciseness: 7, Readability: 8\n",
        "\n",
        "    Please provide scores for the given dataset description based on the Evaluation Criteria. Do not include any additional information or comments in your response.\n",
        "    Evaluation Form (scores ONLY):\"\"\""
      ],
      "metadata": {
        "id": "L7fDTSbkakm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_system_message = \"You are a helpful and precise assistant for checking the quality of the dataset description.\"\n",
        "evaluation_prompt = \"\"\"You will be given one tabular dataset description. Your task is to rate the description on 3 metrics.\n",
        "    Please make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed.\n",
        "\n",
        "    Evaluation Criteria:\n",
        "    1. Completeness (1-10) - Evaluates how thoroughly the dataset description covers essential aspects such as the scope of data, query workloads, summary statistics, and possible tasks or applications.\n",
        "    A high score indicates that the description provides a comprehensive overview, including details on dataset size, structure, fields, and potential use cases.\n",
        "    2. Conciseness (1-10) - Measures the efficiency of the dataset description in conveying necessary information without redundancy.\n",
        "    A high score indicates that the description is succinct, avoiding unnecessary details while employing semantic types (e.g., categories, entities) to streamline communication.\n",
        "    3. Readability (1-10) -  Evaluates the logical flow and readability of the dataset description.\n",
        "    A high score suggests that the description progresses logically from one section to the next, creating a coherent and integrated narrative that facilitates understanding of the dataset.\n",
        "\n",
        "    Evaluation Steps:\n",
        "    Read the dataset description carefully and identify the main topic and key points. Assign a score for each criterion on a scale of 1 to 10, where 1 is the lowest and 10 is the highest based on the Evaluation Criteria.\n",
        "\n",
        "    Example 1:\n",
        "    Description: The dataset provides information on alcohol-impaired driving deaths and occupant deaths across various states in the United States. It includes data for 51 states, detailing the number of alcohol-impaired driving deaths and occupant deaths, with values ranging from 0 to 3723 and 0 to 10406, respectively. Each entry also contains the state abbreviation and its geographical coordinates. The dataset is structured with categorical and numerical data types, focusing on traffic safety and casualty statistics. Key attributes include state names, death counts, and location coordinates, making it a valuable resource for analyzing traffic safety trends and issues related to impaired driving.\n",
        "    Evaluation Form (scores ONLY): Completeness: 7, Conciseness: 9, Readability: 9\n",
        "\n",
        "    Example 2:\n",
        "    Description: The dataset provides a comprehensive overview of traffic safety statistics across various states in the United States, specifically focusing on alcohol-impaired driving deaths and occupant deaths. It includes data from 51 unique states, represented by their two-letter postal abbreviations, such as MA (Massachusetts), SD (South Dakota), AK (Alaska), MS (Mississippi), and ME (Maine). Each entry in the dataset captures critical information regarding the number of alcohol-impaired driving deaths and the total occupant deaths resulting from traffic incidents.\n",
        "    The column \"Alcohol-Impaired Driving Deaths\" is represented as an integer, indicating the number of fatalities attributed to alcohol impairment while driving. The dataset reveals a range of values, with the highest recorded number being 2367 deaths in Mississippi, highlighting the severity of the issue in certain regions. In contrast, states like Alaska report significantly lower figures, with only 205 alcohol-impaired driving deaths.\n",
        "    The \"Occupant Deaths\" column also consists of integer values, representing the total number of deaths among vehicle occupants, regardless of the cause. This data spans from 0 to 10406, with Mississippi again showing the highest number of occupant deaths at 6100, which raises concerns about overall traffic safety in the state.\n",
        "    Additionally, the dataset includes a \"Location\" column that provides geographical coordinates for each state, enhancing the spatial understanding of the data. The coordinates are formatted as latitude and longitude pairs, allowing for potential mapping and geographical analysis of traffic safety trends.\n",
        "    Overall, this dataset serves as a valuable resource for researchers, policymakers, and public safety advocates aiming to understand and address the impact of alcohol on driving safety across different states. It highlights the need for targeted interventions and policies to reduce alcohol-impaired driving incidents and improve occupant safety on the roads.\n",
        "    Evaluation Form (scores ONLY): Completeness: 8, Conciseness: 7, Readability: 8\n",
        "\n",
        "    Please provide scores for the given dataset description based on the Evaluation Criteria. Do not include any additional information or comments in your response.\"\"\"\n",
        "\n",
        "\n",
        "evaluation_models = [\n",
        "    \"unsloth/Meta-Llama-3.1-70B-Instruct\",\n",
        "    \"unsloth/Qwen2-72B-Instruct\",\n",
        "]\n",
        "\n",
        "class HFEvaluator:\n",
        "    \"\"\"Evaluates descriptions using a Hugging Face model\"\"\"\n",
        "\n",
        "    def __init__(self, model_name):\n",
        "        if model_name not in generation_models:\n",
        "            raise ValueError(f\"Model '{model_name}' is not in the list of available models. \"\n",
        "                             f\"Choose from: {evaluation_models}\")\n",
        "        self.model_name = model_name\n",
        "        self.template = evaluation_prompt\n",
        "        self.system_message = evaluation_system_message\n",
        "\n",
        "        # Load model + tokenizer\n",
        "        self.model, self.tokenizer = FastLanguageModel.from_pretrained(\n",
        "            model_name=model_name,\n",
        "            max_seq_length=4096,\n",
        "            dtype=None,\n",
        "            load_in_4bit=True,\n",
        "        )\n",
        "\n",
        "        FastLanguageModel.for_inference(self.model)\n",
        "\n",
        "        if \"Qwen\" in model_name:\n",
        "            self.tokenizer.pad_token = \"<|extra_0|>\"\n",
        "            self.tokenizer.eos_token = \"</s>\"\n",
        "            self.tokenizer.bos_token = \"<s>\"\n",
        "\n",
        "            self.eos_ids = [self.tokenizer.eos_token_id]\n",
        "\n",
        "        else:  # LLaMA\n",
        "            self.eos_ids = [\n",
        "                self.tokenizer.eos_token_id,\n",
        "                self.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "            ]\n",
        "\n",
        "    def evaluate_description(self, description):\n",
        "        \"\"\"Evaluates a description given a prompt and temperature\"\"\"\n",
        "\n",
        "        # Build final message content in one step\n",
        "        user_content = (\n",
        "            f\"{self.template}\\n\"\n",
        "            f\"Description: {description}\\n\"\n",
        "            \"Evaluation Form (scores ONLY): \"\n",
        "        )\n",
        "\n",
        "        prompt = self.tokenizer.apply_chat_template(\n",
        "            [\n",
        "                {\"role\": \"system\", \"content\": self.system_message},\n",
        "                {\"role\": \"user\", \"content\": user_content},\n",
        "            ],\n",
        "            tokenize=False,\n",
        "        )\n",
        "\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "        if \"Llama\" in self.model_name or \"Meta-Llama\" in self.model_name:\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=50,\n",
        "                    do_sample=True,\n",
        "                    temperature=0.3,\n",
        "                    num_beams=1,\n",
        "                    eos_token_id=self.eos_ids,\n",
        "                    pad_token_id=self.tokenizer.eos_token_id,\n",
        "                    use_cache=True,\n",
        "                )\n",
        "\n",
        "        else:  # Qwen branch\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=50,\n",
        "                    do_sample=True,\n",
        "                    temperature=0.3,\n",
        "                    eos_token_id=self.eos_ids,\n",
        "                    pad_token_id=self.tokenizer.pad_token_id,\n",
        "                    use_cache=True,\n",
        "                )\n",
        "\n",
        "        # alt: self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        text = self.tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
        "        return text[len(prompt):].strip()\n"
      ],
      "metadata": {
        "id": "RXaeQFS6S1tR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BaseEvaluator:\n",
        "    \"\"\"\n",
        "    Base class implementing a simple evaluation workflow.\n",
        "\n",
        "    Args:\n",
        "        client: LLM client instance\n",
        "        model_name: Model name to use for evaluation\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, client: Any, model_name: str) -> None:\n",
        "        self.client = client\n",
        "        self.model = model_name\n",
        "\n",
        "        self.system_message = \"You are a helpful and precise assistant for checking the quality of the dataset description.\"\n",
        "        self.template = evaluation_prompt\n",
        "\n",
        "    def evaluate(self, description: str) -> str:\n",
        "        \"\"\"\n",
        "        Evaluates the description and returns the raw score string.\n",
        "        \"\"\"\n",
        "\n",
        "        # Build final message content in one step\n",
        "        user_content = (\n",
        "            f\"{self.template}\\n\"\n",
        "            f\"Description: {description}\\n\"\n",
        "            \"Evaluation Form (scores ONLY): \"\n",
        "        )\n",
        "\n",
        "        # Single point where generation happens\n",
        "        response = self.client.chat.completions.create(\n",
        "            model=self.model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": self.system_message},\n",
        "                {\"role\": \"user\",    \"content\": user_content},\n",
        "            ],\n",
        "            temperature=0.3,\n",
        "        )\n",
        "\n",
        "        return response.choices[0].message.content\n",
        "\n",
        "\n",
        "@beartype\n",
        "class LLaMAEvaluator(BaseEvaluator):\n",
        "    \"\"\"\n",
        "    Evaluate descriptions using DeepInfra LLaMA\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        llama_api_key: str = \"\",\n",
        "        model_name: str = \"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n",
        "    ) -> None:\n",
        "        client = openai.OpenAI(\n",
        "            api_key=llama_api_key, base_url=\"https://api.deepinfra.com/v1/openai\"\n",
        "        )\n",
        "        super().__init__(client=client, model_name=model_name)\n"
      ],
      "metadata": {
        "id": "5Uk_6J0yS1pn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AkI2r0mCS1mA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Retrieval Evaluation\n",
        "Purpose: Measures how generated descriptions improve dataset search and findability.\n",
        "\n",
        "- Method: Integrates descriptions into a keyword-based search engine.\n",
        "- Metric: Normalized Discounted Cumulative Gain (NDCG@k).\n",
        "- Evaluates ranking quality by comparing ideal vs. actual search results.\n",
        "- Higher scores mean relevant datasets appear earlier in search results.\n",
        "- Tools: Tested with BM25 (lexical keyword matching) and SPLADE (semantic term expansion)."
      ],
      "metadata": {
        "id": "5tRImKK23_6b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import ndcg_score\n",
        "\n",
        "# Example: relevance scores for 5 datasets\n",
        "true_relevance = [[3, 2, 3, 0, 1]]   # ground truth relevance\n",
        "scores = [[0.9, 0.8, 0.7, 0.2, 0.3]] # model scores\n",
        "\n",
        "# Compute NDCG@5\n",
        "ndcg = ndcg_score(true_relevance, scores, k=5)\n",
        "print(\"NDCG@5:\", ndcg)\n"
      ],
      "metadata": {
        "id": "cnEeIpVV3wFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reference-Based Evaluation\n",
        "Purpose: Compares generated descriptions against existing dataset descriptions.\n",
        "\n",
        "Metrics:\n",
        "METEOR: Accounts for precision, recall, synonyms, and stemming.\n",
        "ROUGE: Measures overlap of n-grams, sequences, and recall.\n",
        "BERTScore: Uses contextual embeddings to assess semantic similarity.\n",
        "Outcome: Determines how closely AutoDDGâ€™s descriptions match human-written ones in wording and meaning."
      ],
      "metadata": {
        "id": "iyWF8FEQ4WbO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "corpus = [\n",
        "    \"health insurance premiums liabilities assets\",\n",
        "    \"citi bike trips 2022\",\n",
        "    \"yellow taxi trip data multiple years\"\n",
        "]\n",
        "tokenized_corpus = [doc.split(\" \") for doc in corpus]\n",
        "bm25 = BM25Okapi(tokenized_corpus)\n",
        "\n",
        "query = \"insurance financial data\"\n",
        "scores = bm25.get_scores(query.split(\" \"))\n",
        "print(scores)\n"
      ],
      "metadata": {
        "id": "LeI7OWiL3wJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# METEOR\n",
        "import nltk\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "\n",
        "reference = \"This dataset contains wind speed and direction measurements.\"\n",
        "candidate = \"Wind speed and direction data collected during 2003.\"\n",
        "print(\"METEOR:\", meteor_score([reference], candidate))\n",
        "\n",
        "# ROUGE\n",
        "from rouge_score import rouge_scorer\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
        "scores = scorer.score(reference, candidate)\n",
        "print(\"ROUGE:\", scores)\n",
        "\n",
        "# BERTScore\n",
        "from bert_score import score\n",
        "cands = [candidate]\n",
        "refs = [reference]\n",
        "P, R, F1 = score(cands, refs, lang=\"en\", verbose=True)\n",
        "print(\"BERTScore F1:\", F1.mean().item())\n"
      ],
      "metadata": {
        "id": "Rv_azrFj3wML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import math\n",
        "from typing import Dict, Iterable, List\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from beartype import beartype\n",
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "\n",
        "@beartype\n",
        "def compute_dcg(relevances: Iterable[float], p: int) -> float:\n",
        "    \"\"\"\n",
        "    Discounted cumulative gain at rank p\n",
        "\n",
        "    Args:\n",
        "        relevances: Relevance scores\n",
        "        p: Cut-off rank\n",
        "\n",
        "    Returns:\n",
        "        DCG value\n",
        "    \"\"\"\n",
        "\n",
        "    dcg = 0.0\n",
        "    for index, relevance in enumerate(relevances):\n",
        "        if index >= p:\n",
        "            break\n",
        "        dcg += (2**relevance - 1) / math.log2(index + 2)\n",
        "    return dcg\n",
        "\n",
        "\n",
        "@beartype\n",
        "def compute_avg_single_Q(\n",
        "    stats: Dict[str, Dict[str, Dict[str, List[float]]]],\n",
        "    description_version_key: str,\n",
        "    Q_key: str,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Average nested metrics into a DataFrame by index version\n",
        "\n",
        "    Args:\n",
        "        stats: Nested metrics dict\n",
        "        description_version_key: Key selecting description version\n",
        "        Q_key: Key selecting metric group\n",
        "\n",
        "    Returns:\n",
        "        DataFrame of averaged metrics\n",
        "    \"\"\"\n",
        "\n",
        "    averages: Dict[str, Dict[str, float]] = {}\n",
        "    ndcg_dicts = stats[description_version_key][Q_key]\n",
        "    for index_version, ndcg_metric in ndcg_dicts.items():\n",
        "        averages[index_version] = {\n",
        "            key: float(np.average(scores)) for key, scores in ndcg_metric.items()\n",
        "        }\n",
        "    return pd.DataFrame(averages)\n",
        "\n",
        "\n",
        "@beartype\n",
        "def compute_ndcg(\n",
        "    retrieved_relevances: Iterable[float], ideal_relevances: Iterable[float], p: int\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Normalised DCG at rank p\n",
        "\n",
        "    Args:\n",
        "        retrieved_relevances: Relevances by retrieved order\n",
        "        ideal_relevances: Relevances by ideal order\n",
        "        p: Cut-off rank\n",
        "\n",
        "    Returns:\n",
        "        nDCG value\n",
        "    \"\"\"\n",
        "\n",
        "    dcg = compute_dcg(retrieved_relevances, p)\n",
        "    idcg = compute_dcg(ideal_relevances, p)\n",
        "    return dcg / idcg if idcg > 0 else 0.0\n",
        "\n",
        "\n",
        "@beartype\n",
        "def downstream_task_rank(\n",
        "    documents: List[str],\n",
        "    query: str,\n",
        "    relevances: List[float],\n",
        "    ks: Iterable[int],\n",
        "    debug: bool = False,\n",
        ") -> Dict[int, Dict[str, float]]:\n",
        "    \"\"\"\n",
        "    BM25 ranking with nDCG@k over the inputted documents\n",
        "\n",
        "    Args:\n",
        "        documents: List of document texts\n",
        "        query: Query string\n",
        "        relevances: Ground-truth relevance scores\n",
        "        ks: List of cut-off values\n",
        "        debug: Print debug output if True\n",
        "\n",
        "    Returns:\n",
        "        Mapping k -> metrics\n",
        "    \"\"\"\n",
        "\n",
        "    def _compute_ndcg(relevance_true: List[float], relevance_test: List[float], k: int) -> float:\n",
        "        ideal_dcg = np.sum(np.array(relevance_true) / np.log2(np.arange(2, k + 2)))\n",
        "        dcg = np.sum(np.array(relevance_test) / np.log2(np.arange(2, k + 2)))\n",
        "        return float(dcg / ideal_dcg)\n",
        "\n",
        "    tokenized_corpus = [doc.lower().split() for doc in documents]\n",
        "    bm25 = BM25Okapi(tokenized_corpus)\n",
        "    tokenized_query = query.lower().split()\n",
        "    if debug:\n",
        "        print(tokenized_query)\n",
        "\n",
        "    scores = bm25.get_scores(tokenized_query)\n",
        "    sorted_indices = np.argsort(scores)[::-1]\n",
        "    sorted_rel_true = sorted(relevances, reverse=True)\n",
        "    sorted_rel_test = np.array(relevances)[sorted_indices].tolist()\n",
        "\n",
        "    results: Dict[int, Dict[str, float]] = {}\n",
        "    for k in ks:\n",
        "        ndcg = _compute_ndcg(sorted_rel_true[:k], sorted_rel_test[:k], k)\n",
        "        results[k] = {\"ndcg\": ndcg}\n",
        "    return results"
      ],
      "metadata": {
        "id": "mjDL6rzH6YHp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}