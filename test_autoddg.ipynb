{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNxB+E0IxVCjUopq0ujAHHW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elizabethavargas/Dataset-Description-Generation/blob/main/test_autoddg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/VIDA-NYU/AutoDDG@main\n",
        "\n",
        "import pandas as pd\n",
        "from openai import OpenAI\n",
        "\n",
        "from autoddg import AutoDDG, GPTEvaluator, LLaMAEvaluator\n",
        "from autoddg.utils import get_sample"
      ],
      "metadata": {
        "id": "xbUDtDG0hyOR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56b0b5cc-5f9d-497f-a7e1-fa387f0cce69"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/VIDA-NYU/AutoDDG@main\n",
            "  Cloning https://github.com/VIDA-NYU/AutoDDG (to revision main) to /tmp/pip-req-build-go3xv_jt\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/VIDA-NYU/AutoDDG /tmp/pip-req-build-go3xv_jt\n",
            "  Resolved https://github.com/VIDA-NYU/AutoDDG to commit 5f26a43b216d2ee4079b8e4969397a12d414db95\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: datamart_profiler==0.11 in /usr/local/lib/python3.12/dist-packages (from autoddg==0.1.0.dev0) (0.11)\n",
            "Requirement already satisfied: fastembed==0.5.1 in /usr/local/lib/python3.12/dist-packages (from autoddg==0.1.0.dev0) (0.5.1)\n",
            "Requirement already satisfied: nltk==3.9.1 in /usr/local/lib/python3.12/dist-packages (from autoddg==0.1.0.dev0) (3.9.1)\n",
            "Requirement already satisfied: numpy>=2.2.2 in /usr/local/lib/python3.12/dist-packages (from autoddg==0.1.0.dev0) (2.3.5)\n",
            "Requirement already satisfied: openai==1.61.0 in /usr/local/lib/python3.12/dist-packages (from autoddg==0.1.0.dev0) (1.61.0)\n",
            "Requirement already satisfied: pandas>=2.2.3 in /usr/local/lib/python3.12/dist-packages (from autoddg==0.1.0.dev0) (2.3.3)\n",
            "Requirement already satisfied: rank-bm25==0.2.2 in /usr/local/lib/python3.12/dist-packages (from autoddg==0.1.0.dev0) (0.2.2)\n",
            "Requirement already satisfied: scikit-learn==1.6.1 in /usr/local/lib/python3.12/dist-packages (from autoddg==0.1.0.dev0) (1.6.1)\n",
            "Requirement already satisfied: beartype>=0.18.5 in /usr/local/lib/python3.12/dist-packages (from autoddg==0.1.0.dev0) (0.22.6)\n",
            "Requirement already satisfied: pyyaml==6.0.2 in /usr/local/lib/python3.12/dist-packages (from autoddg==0.1.0.dev0) (6.0.2)\n",
            "Requirement already satisfied: opentelemetry-api in /usr/local/lib/python3.12/dist-packages (from datamart_profiler==0.11->autoddg==0.1.0.dev0) (1.37.0)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.12/dist-packages (from datamart_profiler==0.11->autoddg==0.1.0.dev0) (0.23.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.12/dist-packages (from datamart_profiler==0.11->autoddg==0.1.0.dev0) (2.9.0.post0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from datamart_profiler==0.11->autoddg==0.1.0.dev0) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from datamart_profiler==0.11->autoddg==0.1.0.dev0) (2.32.4)\n",
            "Requirement already satisfied: datamart-geo<0.4,>=0.2.3 in /usr/local/lib/python3.12/dist-packages (from datamart_profiler==0.11->autoddg==0.1.0.dev0) (0.3.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.20 in /usr/local/lib/python3.12/dist-packages (from fastembed==0.5.1->autoddg==0.1.0.dev0) (0.36.0)\n",
            "Requirement already satisfied: loguru<0.8.0,>=0.7.2 in /usr/local/lib/python3.12/dist-packages (from fastembed==0.5.1->autoddg==0.1.0.dev0) (0.7.3)\n",
            "Requirement already satisfied: mmh3<5.0.0,>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from fastembed==0.5.1->autoddg==0.1.0.dev0) (4.1.0)\n",
            "Requirement already satisfied: onnxruntime!=1.20.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from fastembed==0.5.1->autoddg==0.1.0.dev0) (1.23.2)\n",
            "Requirement already satisfied: pillow<11.0.0,>=10.3.0 in /usr/local/lib/python3.12/dist-packages (from fastembed==0.5.1->autoddg==0.1.0.dev0) (10.4.0)\n",
            "Requirement already satisfied: py-rust-stemmers<0.2.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from fastembed==0.5.1->autoddg==0.1.0.dev0) (0.1.5)\n",
            "Requirement already satisfied: tokenizers<1.0,>=0.15 in /usr/local/lib/python3.12/dist-packages (from fastembed==0.5.1->autoddg==0.1.0.dev0) (0.22.1)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.66 in /usr/local/lib/python3.12/dist-packages (from fastembed==0.5.1->autoddg==0.1.0.dev0) (4.67.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk==3.9.1->autoddg==0.1.0.dev0) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk==3.9.1->autoddg==0.1.0.dev0) (1.5.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai==1.61.0->autoddg==0.1.0.dev0) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai==1.61.0->autoddg==0.1.0.dev0) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai==1.61.0->autoddg==0.1.0.dev0) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai==1.61.0->autoddg==0.1.0.dev0) (0.12.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai==1.61.0->autoddg==0.1.0.dev0) (2.12.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai==1.61.0->autoddg==0.1.0.dev0) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai==1.61.0->autoddg==0.1.0.dev0) (4.15.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.6.1->autoddg==0.1.0.dev0) (1.16.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.6.1->autoddg==0.1.0.dev0) (3.6.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.3->autoddg==0.1.0.dev0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.3->autoddg==0.1.0.dev0) (2025.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai==1.61.0->autoddg==0.1.0.dev0) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai==1.61.0->autoddg==0.1.0.dev0) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai==1.61.0->autoddg==0.1.0.dev0) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.61.0->autoddg==0.1.0.dev0) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.20->fastembed==0.5.1->autoddg==0.1.0.dev0) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.20->fastembed==0.5.1->autoddg==0.1.0.dev0) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.20->fastembed==0.5.1->autoddg==0.1.0.dev0) (25.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.20->fastembed==0.5.1->autoddg==0.1.0.dev0) (1.2.0)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.12/dist-packages (from onnxruntime!=1.20.0,>=1.17.0->fastembed==0.5.1->autoddg==0.1.0.dev0) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime!=1.20.0,>=1.17.0->fastembed==0.5.1->autoddg==0.1.0.dev0) (25.9.23)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime!=1.20.0,>=1.17.0->fastembed==0.5.1->autoddg==0.1.0.dev0) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime!=1.20.0,>=1.17.0->fastembed==0.5.1->autoddg==0.1.0.dev0) (1.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai==1.61.0->autoddg==0.1.0.dev0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai==1.61.0->autoddg==0.1.0.dev0) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai==1.61.0->autoddg==0.1.0.dev0) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil->datamart_profiler==0.11->autoddg==0.1.0.dev0) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->datamart_profiler==0.11->autoddg==0.1.0.dev0) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->datamart_profiler==0.11->autoddg==0.1.0.dev0) (2.5.0)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api->datamart_profiler==0.11->autoddg==0.1.0.dev0) (8.7.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api->datamart_profiler==0.11->autoddg==0.1.0.dev0) (3.23.0)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.12/dist-packages (from coloredlogs->onnxruntime!=1.20.0,>=1.17.0->fastembed==0.5.1->autoddg==0.1.0.dev0) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime!=1.20.0,>=1.17.0->fastembed==0.5.1->autoddg==0.1.0.dev0) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run AutoDDG on a random sample of datasets\n"
      ],
      "metadata": {
        "id": "5HYN3ONk5Vhm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datasets = pd.read_pickle('datasets.pkl')\n",
        "datasets[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZnL-PrD3w4A",
        "outputId": "c0eb0df4-b282-4e0b-e9de-ba1cd113a630"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dataset_id': 'npwk-bcm6',\n",
              " 'data_example': {'school_year': '2006-2007',\n",
              "  'report_type': 'Citywide',\n",
              "  'program': 'GENERAL EDUCATION',\n",
              "  'grade_or_service_category': 'Kindergarten',\n",
              "  'average_class_size': '20.7'},\n",
              " 'dataset_name': 'Class Size Report (2006-2007)',\n",
              " 'category': 'Education',\n",
              " 'description': 'For schools with students in any grades between Kindergarten and 9th grade (where 9th grade is the termination grade for the school), class size is reported by four program areas: general education, special education self-contained class, collaborative team teaching and gifted and talented self-contained class. Within each program area class size is reported by grade or service category, which indicates how a special education self-contained class is delivered. Class size is calculated by dividing the number of students in a program and grade by the number of official classes in that program and grade.\\nThe following data is excluded from all the reports: District 75 schools, bridge classes which span more than one grade, classes with fewer than five students (for other than special education self-contained classes) and classes with one student (for special education self-contained classes). On the summary reports programs and grades with three or fewer classes are excluded from the citywide, borough and region reports and programs and grades with one class are excluded from the district report. For schools with students in any grades between 9th and 12th grade (where 9th grade is not the termination grade for the school), class size is reported by two program areas: general education and special education. For general education students class size is reported by grade for each core subject area: English, Math, Science and Social Studies. For special education students with a self-contained program recommendation, class size is reported by service category (self-contained or mainstream) for each core subject area. Since high school classes may contain students in multiple grades and programs, class size is calculated by taking a weighted average of all the classes in a core subject area with students in a particular grade or program. For example, there are 75 ninth graders enrolled at a high school. 25 ninth graders attend a Math class with 28 students, a second group of 25 ninth graders attend a Math class with 25 students, and a third group of 25 ninth graders attend a Math class with 30 students. Average class size for ninth grade Math equals: (25x28 + 25x25 + 25x30)/75 = 27.7.\\nThe Pupil Teacher Ratio is also provided on the school level report. Pupil Teacher Ratio is another means to evaluate the instructional resources provided at a school. Pupil Teacher Ratio for All Students is calculated by dividing the number of students at a school by the number of full-time equivalent teachers, including both teachers in classes taught by two teachers, “cluster” teachers providing instruction in specialized topics like art or science, and teachers providing special education instruction. Pupil Teacher Ratio Excluding Special Education is calculated by dividing the number of non-special education students at a school by the number of full-time equivalent non-special education teachers.',\n",
              " 'agency': 'Department of Education (DOE)',\n",
              " 'tags': ['school', 'class size'],\n",
              " 'column_info': {}}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import pickle\n",
        "\n",
        "sampled_datasets = random.sample(datasets, 500)\n",
        "\n",
        "with open('sampled_datasets.pkl', 'wb') as f:\n",
        "    pickle.dump(sampled_datasets, f)"
      ],
      "metadata": {
        "id": "V-PRVL0r4BhB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "key = 'BY4N2cgkwH2dUUsAJPLggIjAzg0il2y5'\n",
        "client = OpenAI(\n",
        "            api_key=key, base_url=\"https://api.deepinfra.com/v1/openai\"\n",
        "        )\n",
        "auto_ddg = AutoDDG(client=client, model_name=\"meta-llama/Meta-Llama-3-8B-Instruct\", description_temperature=0.0)\n"
      ],
      "metadata": {
        "id": "HLpm-DJZ_J5O"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_results = []\n",
        "\n",
        "for dataset in sampled_datasets:\n",
        "  # get input info\n",
        "  title = dataset['dataset_name']\n",
        "  original_description = dataset['description']\n",
        "  data_dict = dataset['data_example']\n",
        "  data_df = pd.DataFrame([data_dict])\n",
        "  id = dataset['dataset_id']\n",
        "\n",
        "  # Sample rows\n",
        "  sample_df, dataset_sample = get_sample(data_df, sample_size=1)\n",
        "\n",
        "  # Generate profiles\n",
        "  basic_profile, structural_profile = auto_ddg.profile_dataframe(data_df)\n",
        "  semantic_profile_details = auto_ddg.analyze_semantics(sample_df)\n",
        "  semantic_profile = \"\\n\".join(\n",
        "      section for section in [structural_profile, semantic_profile_details] if section\n",
        "  )\n",
        "\n",
        "  # Generate topic\n",
        "  data_topic = auto_ddg.generate_topic(\n",
        "      title=title,\n",
        "      original_description=original_description,\n",
        "      dataset_sample=dataset_sample,\n",
        "  )\n",
        "\n",
        "  # General description\n",
        "  prompt, description = auto_ddg.describe_dataset(\n",
        "      dataset_sample=dataset_sample,\n",
        "      dataset_profile=basic_profile,\n",
        "      use_profile=True,\n",
        "      semantic_profile=semantic_profile,\n",
        "      use_semantic_profile=True,\n",
        "      data_topic=data_topic,\n",
        "      use_topic=True,\n",
        "  )\n",
        "\n",
        "  processed_results.append({\n",
        "      'id': id,\n",
        "      'prompt': prompt,\n",
        "      'description': description\n",
        "  })"
      ],
      "metadata": {
        "id": "hpMBzJ7aAsay"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cb98c63"
      },
      "source": [
        "### Parallelizing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import concurrent.futures\n",
        "import pandas as pd\n",
        "import time\n",
        "from openai import OpenAI\n",
        "from autoddg import AutoDDG\n",
        "from autoddg.utils import get_sample\n",
        "\n",
        "\n",
        "# ---- Worker Function -------------------------------------------------\n",
        "\n",
        "def process_single_dataset(dataset_entry, api_key, max_retries=3, retry_delay=2):\n",
        "    \"\"\"Process one dataset safely in an isolated process.\"\"\"\n",
        "\n",
        "    # retry loop to handle rate-limit errors or corrupted responses\n",
        "    for attempt in range(1, max_retries + 1):\n",
        "        try:\n",
        "            # Create fresh, isolated client\n",
        "            local_client = OpenAI(\n",
        "                api_key=api_key,\n",
        "                base_url=\"https://api.deepinfra.com/v1/openai\"\n",
        "            )\n",
        "\n",
        "            local_auto_ddg = AutoDDG(\n",
        "                client=local_client,\n",
        "                model_name=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
        "                description_temperature=0.0\n",
        "            )\n",
        "\n",
        "            # extract dataset info\n",
        "            title = dataset_entry[\"dataset_name\"]\n",
        "            original_description = dataset_entry[\"description\"]\n",
        "            data_dict = dataset_entry[\"data_example\"]\n",
        "            dataset_id = dataset_entry[\"dataset_id\"]\n",
        "\n",
        "            # dataframe\n",
        "            data_df = pd.DataFrame([data_dict])\n",
        "\n",
        "            # sample rows\n",
        "            sample_df, dataset_sample = get_sample(data_df, sample_size=1)\n",
        "\n",
        "            # profiles\n",
        "            basic_profile, structural_profile = local_auto_ddg.profile_dataframe(data_df)\n",
        "            semantic_profile_details = local_auto_ddg.analyze_semantics(sample_df)\n",
        "\n",
        "            semantic_profile = \"\\n\".join(\n",
        "                section for section in [structural_profile, semantic_profile_details]\n",
        "                if section\n",
        "            )\n",
        "\n",
        "            # Topic\n",
        "            data_topic = local_auto_ddg.generate_topic(\n",
        "                title=title,\n",
        "                original_description=original_description,\n",
        "                dataset_sample=dataset_sample,\n",
        "            )\n",
        "\n",
        "            # Description\n",
        "            prompt, description = local_auto_ddg.describe_dataset(\n",
        "                dataset_sample=dataset_sample,\n",
        "                dataset_profile=basic_profile,\n",
        "                use_profile=True,\n",
        "                semantic_profile=semantic_profile,\n",
        "                use_semantic_profile=True,\n",
        "                data_topic=data_topic,\n",
        "                use_topic=True,\n",
        "            )\n",
        "\n",
        "            # SUCCESS\n",
        "            return {\n",
        "                \"id\": dataset_id,\n",
        "                \"prompt\": prompt,\n",
        "                \"description\": description,\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(\n",
        "                f\"[{dataset_entry['dataset_id']}] Attempt {attempt}/{max_retries} failed: {e}\"\n",
        "            )\n",
        "            if attempt == max_retries:\n",
        "                # give up\n",
        "                return {\n",
        "                    \"id\": dataset_entry[\"dataset_id\"],\n",
        "                    \"error\": str(e),\n",
        "                    \"prompt\": None,\n",
        "                    \"description\": None\n",
        "                }\n",
        "\n",
        "            time.sleep(retry_delay)  # backoff\n",
        "\n",
        "\n",
        "# ---- Parallel Execution (ProcessPool) --------------------------------\n",
        "\n",
        "processed_results = []\n",
        "\n",
        "with concurrent.futures.ProcessPoolExecutor(max_workers=10) as executor:\n",
        "    future_to_dataset = {\n",
        "        executor.submit(process_single_dataset, dataset, key): dataset\n",
        "        for dataset in sampled_datasets\n",
        "    }\n",
        "\n",
        "    for future in concurrent.futures.as_completed(future_to_dataset):\n",
        "        dataset = future_to_dataset[future]\n",
        "        result = future.result()\n",
        "        processed_results.append(result)\n",
        "\n",
        "# Summary output\n",
        "print(f\"Processed {len(processed_results)} datasets.\\n\")\n",
        "\n",
        "# Show first successful result\n",
        "first_ok = next((r for r in processed_results if r.get(\"prompt\")), None)\n",
        "if first_ok:\n",
        "    print(\"First processed dataset with valid results:\")\n",
        "    print(first_ok)\n",
        "else:\n",
        "    print(\"No successful dataset found.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBLOCYKTJyxw",
        "outputId": "ab448bcf-9cc2-48c3-b7a1-aeef7f704600"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[r6ub-zhff] Attempt 1/3 failed: 'dict' object has no attribute 'lower'\n",
            "[7umd-hdjb] Attempt 1/3 failed: 'bool' object has no attribute 'get'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datamart_profiler.core:Unmatched latitude columns: ['residents_hispanic_or_latino']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[rdjw-z878] Attempt 1/3 failed: 'bool' object has no attribute 'get'\n",
            "[r6ub-zhff] Attempt 2/3 failed: 'str' object has no attribute 'get'\n",
            "[m64b-i6yz] Attempt 1/3 failed: 'bool' object has no attribute 'get'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datamart_profiler.core:Unmatched latitude columns: ['residents_hispanic_or_latino']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[vf4p-p8ui] Attempt 1/3 failed: 'bool' object has no attribute 'get'\n",
            "[ya22-5bh7] Attempt 1/3 failed: 'NoneType' object has no attribute 'get'\n",
            "[ucdy-byxd] Attempt 1/3 failed: 'dict' object has no attribute 'lower'\n",
            "[wtqm-fd2z] Attempt 1/3 failed: 'str' object has no attribute 'get'\n",
            "[rhe8-mgbb] Attempt 1/3 failed: 'dict' object has no attribute 'lower'\n",
            "[pjs3-c3z5] Attempt 1/3 failed: 'bool' object has no attribute 'get'\n",
            "[r6ub-zhff] Attempt 3/3 failed: Error code: 400 - {'error': {'message': 'Input too long'}}\n",
            "[usc3-8zwd] Attempt 1/3 failed: 'str' object has no attribute 'get'\n",
            "[yeba-ynb5] Attempt 1/3 failed: Error code: 400 - {'error': {'message': 'Input too long'}}\n",
            "[5kqf-fg3n] Attempt 1/3 failed: 'dict' object has no attribute 'lower'\n",
            "[usc3-8zwd] Attempt 2/3 failed: 'dict' object has no attribute 'lower'\n",
            "[tyv9-j3ti] Attempt 1/3 failed: 'dict' object has no attribute 'lower'\n",
            "[yeba-ynb5] Attempt 2/3 failed: Error code: 400 - {'error': {'message': 'Input too long'}}\n",
            "[m8p6-tp4b] Attempt 1/3 failed: 'bool' object has no attribute 'get'\n",
            "[kvfi-kxcq] Attempt 1/3 failed: 'str' object has no attribute 'get'\n",
            "[9ksk-35jf] Attempt 1/3 failed: 'dict' object has no attribute 'lower'\n",
            "[6bgk-3dad] Attempt 1/3 failed: 'bool' object has no attribute 'get'\n",
            "[usc3-8zwd] Attempt 3/3 failed: Error code: 400 - {'error': {'message': 'Input too long'}}\n",
            "[yeba-ynb5] Attempt 3/3 failed: Error code: 400 - {'error': {'message': 'Input too long'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('autoddg_results.pkl', 'wb') as f:\n",
        "    pickle.dump(processed_results, f)"
      ],
      "metadata": {
        "id": "LDKx4-_tKBq5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}